const path = require('path')
const util = require('util')
const fs = require('fs-extra')
const fileType = require('file-type')
const AWS = require('aws-sdk')
const mime = require('mime-types')

const astat = util.promisify(fs.stat)
const areaddir = util.promisify(fs.readdir)
const areadFile = util.promisify(fs.readFile)

const option = {
  defaultThreshold: 10 * 1024 * 1024, // 10MB
  minUploadThreadCount: 1,
  maxUploadThreadCount: 2000
}

async function S3Upload(uploadConfig) {
  const isExist = await checkFileExistAndReadable(uploadConfig)
  if (!isExist) {
    console.error('Target doesn\'t exist')
    return
  }
  let pwd = process.env.PWD
  let config = fs.readJsonSync(uploadConfig)

  let targetDir = path.resolve(pwd, config.src_dir)
  let prefix = config['key_prefix']

  console.time(`list [${targetDir}] all files`)
  const files = await fileList(targetDir)
  console.timeEnd(`list [${targetDir}] all files`)

  if (!files.length) {
    console.log('Target finder doesn\'t contain any files')
    return
  }

  Promise.all(files.map(async item => {
    let extraPath = item.filename.replace(targetDir, '')
    let targetPath = path.join(prefix, extraPath)
    console.time(`Uploading ${item.filename} => ${targetPath}`)
    console.log(item.buffer, item.mime)
    await uploadFile(targetPath, item.buffer, item.mime)
    console.timeEnd(`Uploading ${item.filename} => ${targetPath}`)
  })).then(() => {
    console.log('all files are uploaded')
  }).catch(e => {
    console.error(`something wrong: ${e.stack}`)
  })
}

/**
 * check finder files, return file Content-Type & buffer
 * @param dir
 * @returns {Promise}
 */
function fileList(dir) {
  return new Promise(resolve => {
    fileReader(dir)
      .then(flatten)
      .then(files => JSON.stringify(files, null, 2))
      .then(data => JSON.parse(data) || [])
      .then(data => {
        resolve(Promise.all(
          data.map(async item => {
            let file = await areadFile(item)
            return {
              filename: item,
              buffer: file,
              mime: mime.lookup(item) || 'application/octet-stream'
            }
          }))
        )
      })
      .catch(console.error)
  })
}

/**
 * recursive search all finder's files
 * @param dir
 * @returns {Promise.<*[]>}
 */
async function fileReader(dir) {
  const files = await areaddir(dir)
  return Promise.all(files
    .map(f => path.join(dir, f))
    .map(async f => {
      const stats = await astat(f)
      return stats.isDirectory() ? fileReader(f) : f
    })
  )
}

function flatten(arr) {
  return arr.reduce((flat, toFlatten) => flat.concat(Array.isArray(toFlatten) ? flatten(toFlatten) : toFlatten), []);
}


/*
  check file exist or not
 */
function checkFileExistAndReadable(path) {
  return new Promise(function(resolve) {
    fs.ensureFile(path, (err) => {
      if (err) {
        console.log(`<LocalUploadConfig> ${err.code === 'ENOENT' ? 'does not exist' : 'is not access'} `)
        resolve(false)
      } else {
        resolve(true)
      }
    })
  })
}

/*
  upload single file to bucket
 */
function uploadFile(key, file, contentType) {
  if (!contentType) return Promise.resolve({})
  const s3 = new AWS.S3({
    region: 'ap-northeast-1'
  })
  const uploadPromise = s3.putObject({
    Bucket: 'ipastbook',
    Body: file,
    Key: key,
    ACL: 'public-read',
    ContentType: contentType
  }).promise()
  return uploadPromise.then(data => {
    return data
  }).catch(e => {
    console.log(e)
    return e
  })
}

module.exports = S3Upload
